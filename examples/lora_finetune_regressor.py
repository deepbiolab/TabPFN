"""
Provides a detailed example of fine-tuning a TabPFNRegressor model using LoRA.

This script demonstrates the complete workflow, including data loading, model
configuration with LoRA injection, a parameter-efficient fine-tuning loop,
and performance evaluation.

Note: We recommend running the fine-tuning scripts on a CUDA-enabled GPU.
"""

from functools import partial

import numpy as np
import sklearn.datasets
import torch
import torch.nn as nn
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm

# --- [LoRA] 1. å¯¼å…¥ loralib ---
try:
    import lora
except ImportError:
    raise ImportError(
        "loralib is not installed. Please install it with 'pip install loralib'"
    )

from tabpfn import TabPFNRegressor
from tabpfn.finetune_utils import clone_model_for_evaluation
from tabpfn.utils import meta_dataset_collator


# --- [LoRA] 2. è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç»Ÿè®¡å¯è®­ç»ƒå‚æ•° ---
def count_trainable_parameters(model):
    """Returns the number of trainable parameters in the model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def prepare_data(config: dict) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Loads, subsets, and splits the Bike Sharing Demand dataset."""
    print("--- 1. Data Preparation ---")
    bike_sharing = sklearn.datasets.fetch_openml(
        name="Bike_Sharing_Demand", version=2, as_frame=True, parser="auto"
    )

    X_df = bike_sharing.data
    y_df = bike_sharing.target
    X_numeric = X_df.select_dtypes(include=np.number)
    X_all, y_all = X_numeric.values, y_df.values

    rng = np.random.default_rng(config["random_seed"])
    num_samples_to_use = min(config["num_samples_to_use"], len(y_all))
    indices = rng.choice(np.arange(len(y_all)), size=num_samples_to_use, replace=False)
    X, y = X_all[indices], y_all[indices]

    splitter = partial(
        train_test_split,
        test_size=config["valid_set_ratio"],
        random_state=config["random_seed"],
    )
    X_train, X_test, y_train, y_test = splitter(X, y)

    print(
        f"Loaded and split data: {X_train.shape[0]} train, {X_test.shape[0]} test samples."
    )
    print("---------------------------\n")
    return X_train, X_test, y_train, y_test


def setup_regressor(config: dict) -> tuple[TabPFNRegressor, dict]:
    """Initializes the TabPFN regressor and its configuration."""
    print("--- 2. Model Setup ---")
    regressor_config = {
        "ignore_pretraining_limits": True,
        "device": config["device"],
        "n_estimators": 1,  # ä½¿ç”¨å•ä¸ªæ¨¡å‹ä»¥ç®€åŒ–å¾®è°ƒ
        "random_state": config["random_seed"],
        "inference_precision": torch.float32,
    }
    # æ³¨æ„: differentiable_input=True åœ¨å¾®è°ƒæ—¶å¯èƒ½æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºå®ƒä¼šå½±å“æ¨¡å‹å†…éƒ¨çš„å›¾æ„å»º
    regressor = TabPFNRegressor(
        **regressor_config, fit_mode="batched", differentiable_input=True
    )

    print(f"Using device: {config['device']}")
    print("----------------------\n")
    return regressor, regressor_config


def evaluate_regressor(
    regressor: TabPFNRegressor,
    eval_config: dict,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
) -> tuple[float, float, float]:
    """Evaluates the regressor's performance on the test set."""
    # åœ¨è¯„ä¼°å‰ï¼Œéœ€è¦å°†LoRAæƒé‡åˆå¹¶ï¼Œæˆ–è€…åœ¨è¯„ä¼°æ¨¡å¼ä¸‹è¿è¡Œ
    # clone_model_for_evaluation ä¼šåˆ›å»ºä¸€ä¸ªå¹²å‡€çš„å‰¯æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹å‰¯æœ¬ä¹Ÿè¿›è¡Œæ“ä½œ
    # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–ä¸€ä¸‹ï¼Œç›´æ¥åœ¨å¾®è°ƒåçš„æ¨¡å‹ä¸Šè¯„ä¼°
    
    # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œloralibä¼šè‡ªåŠ¨å¤„ç†
    regressor.model_.eval()
    
    # æˆ‘ä»¬å¯ä»¥é€‰æ‹©åˆå¹¶æƒé‡è¿›è¡Œè¯„ä¼°ï¼Œè¿™æ ·é€Ÿåº¦æ›´å¿«ï¼Œä¸”ä¸éœ€è¦ loralib
    # å¦‚æœä¸åˆå¹¶ï¼Œloraå±‚ä¼šåœ¨å‰å‘ä¼ æ’­æ—¶è®¡ç®—ï¼Œæ•ˆæœä¸€æ ·ä½†ç¨æ…¢
    # lora.merge_lora_weights(regressor.model_) # å¯é€‰æ­¥éª¤

    try:
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†å…‹éš†æ¨¡å‹ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å¾®è°ƒåçš„regressor
        # ä¸ºäº†ä¿è¯è¯„ä¼°çš„å…¬å¹³æ€§ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥åœ¨ä¸€ä¸ªå¹²å‡€çš„ã€åˆå¹¶äº†æƒé‡çš„æ¨¡å‹ä¸Šè¯„ä¼°
        # ä½†ä¸ºäº†ç®€åŒ–ç¤ºä¾‹ï¼Œæˆ‘ä»¬ç›´æ¥åœ¨å½“å‰æ¨¡å‹ä¸Šè¯„ä¼°
        
        # ä¸ºäº†æ›´å‡†ç¡®çš„è¯„ä¼°ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¹²å‡€çš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬é‡æ–° fit ä¸€ä¸‹
        # è¿™ç¡®ä¿äº†è¯„ä¼°æ—¶çš„å†…éƒ¨çŠ¶æ€æ˜¯åŸºäºå®Œæ•´çš„è®­ç»ƒæ•°æ®çš„
        eval_regressor = clone_model_for_evaluation(regressor, eval_config, TabPFNRegressor)
        eval_regressor.fit(X_train, y_train)
        
        predictions = eval_regressor.predict(X_test)
        mse = mean_squared_error(y_test, predictions)
        mae = mean_absolute_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)
    except Exception as e:
        print(f"An error occurred during evaluation: {e}")
        mse, mae, r2 = np.nan, np.nan, np.nan

    return mse, mae, r2


def main() -> None:
    """Main function to configure and run the LoRA finetuning workflow."""
    # --- Master Configuration ---
    config = {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "num_samples_to_use": 10_000, # ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„å€¼ä»¥åŠ é€Ÿç¤ºä¾‹
        "random_seed": 42,
        "valid_set_ratio": 0.3,
        "n_inference_context_samples": 1024, # ä¸Šä¸‹æ–‡å¤§å°
    }
    
    # --- [LoRA] 3. æ·»åŠ LoRAç›¸å…³çš„è¶…å‚æ•° ---
    config["finetuning"] = {
        "epochs": 5, # è®­ç»ƒæ›´å¤šè½®æ¬¡ä»¥è§‚å¯ŸLoRAæ•ˆæœ
        # LoRAé€šå¸¸å¯ä»¥ä½¿ç”¨æ¯”å…¨é‡å¾®è°ƒæ›´é«˜çš„å­¦ä¹ ç‡
        "learning_rate": 1e-4, 
        "meta_batch_size": 1,
        "batch_size": 1024, # ä¿æŒä¸ä¸Šä¸‹æ–‡å¤§å°ä¸€è‡´
        
        # LoRA-specific hyperparameters
        "lora_r": 8,           # LoRAçš„ç§©
        "lora_alpha": 16,      # LoRAçš„ç¼©æ”¾å› å­
        "lora_dropout": 0.1,   # LoRAå±‚çš„Dropout
    }
    # ç¡®ä¿batch_sizeä¸è¶…è¿‡å®é™…è®­ç»ƒæ ·æœ¬æ•°
    config["finetuning"]["batch_size"] = int(
        min(
            config["n_inference_context_samples"],
            config["num_samples_to_use"] * (1 - config["valid_set_ratio"]),
        )
    )

    # --- Setup Data, Model, and Dataloader ---
    X_train, X_test, y_train, y_test = prepare_data(config)
    regressor, regressor_config = setup_regressor(config)

    # å®˜æ–¹è„šæœ¬ä¸­ï¼Œæ¨¡å‹åœ¨ get_preprocessed_datasets æ—¶æ‰çœŸæ­£è¢«åˆå§‹åŒ–
    # æˆ‘ä»¬éœ€è¦åœ¨å®ƒè¢«åˆå§‹åŒ–ä¹‹åå†æ³¨å…¥LoRA
    splitter = partial(train_test_split, test_size=config["valid_set_ratio"])
    training_datasets = regressor.get_preprocessed_datasets(
        X_train, y_train, splitter, max_data_size=config["finetuning"]["batch_size"]
    )
    
    # ä»regressorå®ä¾‹ä¸­è·å–åº•å±‚æ¨¡å‹
    # åœ¨TabPFN 2.0+ç‰ˆæœ¬ä¸­ï¼Œæ¨¡å‹å®ä¾‹æ˜¯ .model_
    model = regressor.model_

    # --- [LoRA] 4. å¯¹æ¨¡å‹è¿›è¡ŒLoRAæ”¹é€  ---
    print("\n--- Injecting LoRA layers ---")
    original_params = count_trainable_parameters(model)

    # éå†æ‰€æœ‰æ¨¡å—ï¼Œå°†ç¬¦åˆæ¡ä»¶çš„ nn.Linear æ›¿æ¢ä¸º lora.Linear
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # æ’é™¤è§£ç å¤´ï¼Œé€šå¸¸æˆ‘ä»¬ä¸å¸Œæœ›å¯¹æœ€åçš„è¾“å‡ºå±‚åº”ç”¨LoRA
            if 'decoder' in name:
                print(f"  Skipping LoRA for decoder layer: {name}")
                continue

            parent_name = '.'.join(name.split('.')[:-1])
            layer_name = name.split('.')[-1]
            parent_module = model.get_submodule(parent_name)

            lora_layer = lora.Linear(
                module.in_features,
                module.out_features,
                r=config["finetuning"]["lora_r"],
                lora_alpha=config["finetuning"]["lora_alpha"],
                lora_dropout=config["finetuning"]["lora_dropout"],
                bias=module.bias is not None,
            )
            # å¤åˆ¶åŸå§‹æƒé‡
            lora_layer.weight = module.weight
            if module.bias is not None:
                lora_layer.bias = module.bias
            
            # æ›¿æ¢
            setattr(parent_module, layer_name, lora_layer)
            print(f"  Replaced '{name}' with LoRA layer.")

    # å†»ç»“æ‰€æœ‰éLoRAå‚æ•°
    lora.mark_only_lora_as_trainable(model)

    lora_params = count_trainable_parameters(model)
    print(f"\nOriginal trainable parameters: {original_params}")
    print(f"LoRA trainable parameters: {lora_params} ({(lora_params/original_params)*100:.2f}%)")
    print("---------------------------------\n")

    # åˆ›å»º DataLoader å’Œ Optimizer
    finetuning_dataloader = DataLoader(
        training_datasets,
        batch_size=config["finetuning"]["meta_batch_size"],
        collate_fn=meta_dataset_collator,
    )

    # ä¼˜åŒ–å™¨ç°åœ¨åªä¼šçœ‹åˆ°å¯è®­ç»ƒçš„LoRAå‚æ•°
    optimizer = Adam(model.parameters(), lr=config["finetuning"]["learning_rate"])
    print(
        f"--- Optimizer Initialized for LoRA: Adam, LR: {config['finetuning']['learning_rate']} ---\n"
    )

    eval_config = {
        **regressor_config,
        "inference_config": {
            "SUBSAMPLE_SAMPLES": config["n_inference_context_samples"]
        },
    }

    # --- Finetuning and Evaluation Loop ---
    print("--- 3. Starting Finetuning & Evaluation ---")
    for epoch in range(config["finetuning"]["epochs"] + 1):
        # åˆå§‹è¯„ä¼°ï¼ˆepoch 0ï¼‰
        if epoch == 0:
            status = "Initial (Before Finetuning)"
            mse, mae, r2 = evaluate_regressor(
                regressor, eval_config, X_train, y_train, X_test, y_test
            )
            print(
                f"ğŸ“Š {status} Evaluation | Test MSE: {mse:.4f}, Test MAE: {mae:.4f}, Test R2: {r2:.4f}\n"
            )
            continue
        
        # å¾®è°ƒ
        model.train()  # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        progress_bar = tqdm(finetuning_dataloader, desc=f"Finetuning Epoch {epoch}")
        for data_batch in progress_bar:
            optimizer.zero_grad()
            (
                X_trains_preprocessed,
                X_tests_preprocessed,
                y_trains_znorm,
                y_test_znorm,
                cat_ixs,
                confs,
                raw_space_bardist_,
                znorm_space_bardist_,
                _,
                y_test_raw,
            ) = data_batch

            regressor.raw_space_bardist_ = raw_space_bardist_[0]
            regressor.bardist_ = znorm_space_bardist_[0]
            regressor.fit_from_preprocessed(
                X_trains_preprocessed, y_trains_znorm, cat_ixs, confs
            )
            logits, _, _ = regressor.forward(X_tests_preprocessed)

            loss_fn = znorm_space_bardist_[0]
            y_target = y_test_znorm

            loss = loss_fn(logits, y_target.to(config["device"])).mean()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        # æ¯ä¸ªepochåè¯„ä¼°
        status = f"After Epoch {epoch}"
        mse, mae, r2 = evaluate_regressor(
            regressor, eval_config, X_train, y_train, X_test, y_test
        )
        print(
            f"ğŸ“Š {status} Evaluation | Test MSE: {mse:.4f}, Test MAE: {mae:.4f}, Test R2: {r2:.4f}\n"
        )

    print("--- âœ… LoRA Finetuning Finished ---")


if __name__ == "__main__":
    main()
